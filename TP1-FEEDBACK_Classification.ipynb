{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6  color=#003366> [LEPL1109] - STATISTICS AND DATA SCIENCES <br><br> \n",
    "(PART II) TP 01 - Classification: Eckelmans needs you!   </font> <br><br><br>\n",
    "\n",
    "<font size=5  color=#003366>\n",
    "Prof. D. Hainaut<br>\n",
    "Prof. L. Jacques<br>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Table of content: \n",
    "* 1 - ECKELMANS NEEDS YOU! (25')\n",
    "   - 1.1 - Context\n",
    "   - 1.2 - Import the data\n",
    "   - 1.2 - Data exploration\n",
    "    <br><br>\n",
    "* 2 - FEATURE SELECTION (30')\n",
    "    <br><br>\n",
    "* 3 - FEATURE STANDARDIZATION (45')\n",
    "   - 3.1 - Data import\n",
    "   - 3.2 - Models\n",
    "    <br><br>\n",
    "* 4 - DATA COMPLETION (20')\n",
    "   - 4.1 - \"Keep it all or drop it all\" approach\n",
    "   - 4.2 - Fill with the mean\n",
    "   - 4.3 - Fill with the median\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br> <br>\n",
    "        <font size=6 color=#009999> 1 - ECKELMANS NEEDS YOU! </font> <br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<font size=5 color=#009999> 1.1 CONTEXT </font> <br>\n",
    "<font size=4 color=#009999>\n",
    "PREDICTION OF THE HOUSE QUALITY\n",
    "</font> <br> <br>\n",
    "</font>\n",
    "\n",
    "Eckelmans is hiring... This well known estate agency in Louvain-la-Neuve wants now to become a leader for houses renting in every campus around the world! To achieve their objective, they need a data scientist to develop a tool that predicts the house quality based on some information (features) available in the Eckelmans database. Here is a description of the dataset: <br><br>\n",
    "\n",
    "\n",
    "<table style=\"width:67%\">\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th style=\"width:18%\"> Feature Name</th>\n",
    "                    <th> Description </th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td style=\"color:#003366\"> <b> OverallQual </b> </td>\n",
    "                    <td style=\"color:#003366\"> <b>  Rates the overall quality and finish of the house (from 1-Very Poor to 10-Very Excellent)   </b>      </td>\n",
    "                </tr>                \n",
    "                <tr>\n",
    "                    <td> SalePrice </td>\n",
    "                    <td> Sale price of the property </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> LotFrontage </td>\n",
    "                    <td> Linear feet of street connected to property </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> OverallCond </td>\n",
    "                    <td> Rates the overall condition of the house (from 1-Very Poor to 10-Very Excellent) </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> LotArea </td>\n",
    "                    <td> Lot size in square feet </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> YearBuilt </td>\n",
    "                    <td> Original construction date </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> YearRemodAdd </td>\n",
    "                    <td> Remodel date (same as construction date if no remodeling or additions) </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> MasVnrArea </td>\n",
    "                    <td> Masonry veneer area in square feet </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> BsmtUnfSF </td>\n",
    "                    <td> Unfinished square feet of basement area </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> TotalBsmtSF </td>\n",
    "                    <td> Total square feet of basement area </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> 1stFlrSF </td>\n",
    "                    <td> First Floor square feet </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> 2ndFlrSF </td>\n",
    "                    <td> Second floor square feet </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> GrLivArea </td>\n",
    "                    <td> Above grade (ground) living area square feet </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> TotRmsAbvGrd </td>\n",
    "                    <td> Total rooms above grade (does not include bathrooms) </td>\n",
    "                </tr>\n",
    "                 <tr>\n",
    "                    <td> GarageArea </td>\n",
    "                    <td> Size of garage in square feet </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> OpenPorchSF </td>\n",
    "                    <td> Open porch area in square feet </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> EnclosedPorch </td>\n",
    "                    <td> Enclosed porch area in square feet </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> 3SsnPorch </td>\n",
    "                    <td> Three season porch area in square feet </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> ScreenPorch </td>\n",
    "                    <td> Screen porch area in square feet </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> PoolArea </td>\n",
    "                    <td> Pool area in square feet </td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "    </table>      \n",
    "<br>\n",
    "\n",
    "The objective is to predict the overall quality stored in the column named `OverallQual`.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "<font size=5 color=#009999> 1.2 IMPORT THE DATA </font> <br>\n",
    "<font size=4 color=#009999>\n",
    "BASIC PANDAS TOOLS\n",
    "</font> <br> <br>\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/docs/) is an open-source Python library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Those data structures are designed to handle common data processing operations. <br><br>\n",
    "\n",
    "First, let's import the dataset and display some basic information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# After talking with the business section of Eckelman, we decide to keep 19 selected features.\n",
    "keepFeature = ['OverallQual', 'LotFrontage', 'SalePrice', 'OverallCond', 'LotArea', 'YearBuilt', 'YearRemodAdd', \n",
    "               'MasVnrArea', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd', \n",
    "               'GarageArea', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch','PoolArea']\n",
    "fullData    = pd.read_csv(\"house_prices.csv\")\n",
    "data        = fullData[keepFeature]\n",
    "# Save this reduced dataset for later\n",
    "data.to_csv(\"HousePrices19Features.csv\", index=False)\n",
    "\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "<font size=5 color=#009999> 1.3 - DATA EXPLORATION </font> <br>\n",
    "<font size=4 color=#009999>\n",
    "PLOTLY AS VISUALIZATION TOOL \n",
    "</font> <br> <br>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 1</b> <br>\n",
    "In the following plots, we are going to compare the target, <code>OverallQual</code>, with respect to all the other features. Are there features more interesting than others? <b>Select the three best features</b>, motivate briefly your choice.\n",
    "</div> \n",
    "\n",
    "You can change the variable `cols`, if you want to plot less or more pictures for each row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nothing to do in this cell :) Run me!\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from helping_functions import plot_comparison_target_feature\n",
    "\n",
    "plot_comparison_target_feature(data, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>FEEDBACK</b>  <br>\n",
    "A good feature should be informative to predict the target, i.e. if one knows the value of the feature it provides information about the value of the target. This implies that there is a relation between the feature and the target. This relation can be linear or not. <br><br>\n",
    "    \n",
    "We comment below some graphs: <br>\n",
    "<ol>\n",
    "   <li> <b>OverallQual vs OverallQual</b><br>\n",
    "       Of course, we observe a perfect correlation between OverallQual (the target) and itself. But remember that we do not have access to the target value for the prediction. So this feature should not be selected as a good feature to predict the target. \n",
    "       <img src=\"FeedbackImgs/1_corr1.png\" width = \"300\">\n",
    "       \n",
    "   <li> <b>SalePrice vs OverallQual</b><br>\n",
    "       There is a clear relation between SalePrice and the target. If we assume that this relation is linear, we can say that there is a linear correlation between the two features. This makes the feature relevant for the prediction.<br>\n",
    "       Note that the correlation is positive (if one value is increasing, so is the other). Then, can be a negative correlation relevant for prediction?\n",
    "       <img src=\"FeedbackImgs/1_corr2.png\" width = \"300\">\n",
    "       \n",
    "   <li> <b>OpenPorchSF vs OverallQual</b><br>\n",
    "       Such plot is hard to interpret. Correlation is not straitforward to identify... \n",
    "       <img src=\"FeedbackImgs/1_corr3.png\" width = \"300\">\n",
    "</ol>    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing all the plots is a tedious task, we would like to have all the information on the same graph! Hopefully, we can use the **correlation matrix**, as a tool to visually show all the correlation between features.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 2</b> <br>\n",
    "Check that the three best features you chose in the previous question are the good ones.\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Nothing to do in this cell :) Run me!\n",
    "\n",
    "from helping_functions import plot_correlation_matrix\n",
    "\n",
    "plot_correlation_matrix(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>FEEDBACK</b>  <br>\n",
    "3 interresting remarks:\n",
    "<ol>\n",
    "   <li> <b>The diagonal is always equal to 1.</b> <br>\n",
    "       The correlation with a feature and itself is always one. \n",
    "   <li> <b>The matrix is symmetric.</b> <br>\n",
    "       The order of comparison does not matter, corr(a,b) = corr(b,a).\n",
    "   <li> <b>Either dark red or dark blue values are the most relevant.</b> <br>\n",
    "       Dark blue values correspond to hard negative correlations. It means that if the value increases, the target decreases. Such information is relevant for prediction. In this dataset, we mostly have positive correlations.<br>\n",
    "       Note that the 4 last features (EnclosedProch, 3SnPorch, ScreenPorch, PoolArea) seem not relevant since their correlation with the target is near to zero. \n",
    "</ol>    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size=5 color=#009999> 2 - FEATURE SELECTION  </font> <br>\n",
    "<font size=4 color=#009999> \n",
    "REDUCE THE NUMBER OF FEATURES BY SELECTING THE MOST RELEVANT ONES\n",
    "</font> <br> <br>\n",
    "\n",
    "Modern day datasets are very rich in information due to the simplicity of the data collection process.\n",
    "This makes the data high dimensional and it is quite common to see datasets with hundreds of features. <br>\n",
    "\n",
    "\n",
    "Feature Selection/Extraction aims to reduce the number of dimensions while minimizing information loss. When presented data have very high dimensionality, models usually choke because:\n",
    "   - Training time increases exponentially with number of features. Resources need also to be allocated for uninformative features. \n",
    "   - Models have increasing risk of overfitting with increasing number of features (curse of dimensionality). Uninformative features then act as noise for the machine learning model that can perform terribly poorly.\n",
    "    \n",
    "**Feature selection** is \"the process of selecting a subset of relevant features for processing, without any transformation\". Such methods consider the relationship between features and the target variable to compute the importance of features. In this exercice session, we will quantify this relation by the use of the correlation between each variable and the target variable.\n",
    "\n",
    " \n",
    "\n",
    "**Feature extraction** \"aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features)\". This new reduced set of features should then be able to summarize most of the information contained in the original set of features. A classical way to perform feature extraction is to use a dimensionality reduction tool such as the principal component analysis (PCA).\n",
    "\n",
    " \n",
    "\n",
    "<img src=\"Imgs/dimensionality_reduction.jpg\" width = \"250\">\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 3 </b> <br>\n",
    "You are asked to implement <b>feature selection</b> by selecting the features that are the 5 most correlated - in absolute value - to the target variable.<br>\n",
    "    Do the same with the 12 most correlated features.\n",
    "</div> \n",
    "\n",
    "\n",
    "Note: the function [`corrwith`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corrwith.html) from the <samp>pandas</samp> library may be useful. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"HousePrices19Features.csv\")\n",
    "\n",
    "# Remove features with NaN\n",
    "data.drop([\"LotFrontage\"], axis=1, inplace=True)\n",
    "data.drop([\"MasVnrArea\"], axis=1, inplace=True)\n",
    "\n",
    "\"\"\"--------------------------------------------------\n",
    "Select the N most correlated features\n",
    "\n",
    "INPUT: \n",
    "    - df: data (DataFrame)\n",
    "    - n: number (int)\n",
    "OUTPUT:\n",
    "    - out: N most correlated features (Series)\n",
    "--------------------------------------------------\"\"\"\n",
    "def feature_selection_corr(dataFrame, n):\n",
    "    out = ...\n",
    "    return out\n",
    "\n",
    "# TODO: select the 5 (or 12) most correlated features.\n",
    "datab = feature_selection_corr(data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>FEEDBACK</b>  <br>\n",
    "The features kept are:\n",
    "<ol>\n",
    "   <li> SalePrice\n",
    "   <li> GrLivArea\n",
    "   <li> YearBuilt\n",
    "   <li> GarageArea\n",
    "   <li> YearRemodAdd\n",
    "</ol>    \n",
    "This can been verified manually by checking the values in the correlation matrix above.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size=5 color=#009999> 3 - FEATURE STANDARDIZATION  </font><br>\n",
    "<font size=4 color=#009999>\n",
    "A MATTER OF UNITS...\n",
    "</font> <br> <br>\n",
    "\n",
    "Unfortunately, Eckelman's revenues plummeted, and we are restricted to work with only the following features: `SalePrice` and `LotArea` to predict `OverallQual`.\n",
    "\n",
    "Three of your friends **A**lice, **B**ob and **S**tanley have been assigned the tedious task of Eckelman's classification: given the sale price and the area of a house, they have to predict whether the quality of that house is high or low.\n",
    "\n",
    "They already agreed on the model classifier: a simple K-nearest neigbhors (K-NN) should be enough. However, they strongly disagree on which units to use.\n",
    "\n",
    "- Alice is from the United States of **A**merica and would rather use the imperial system of units. Besides, the original data is given in square feet (sq. ft) and US dollars (USD).\n",
    "- Bob comes from Syldavia, where the currency is the **B**itcoin (XBT) and the surface is expressed in square smoot.\n",
    "- Stanley does not want to pick a side: he chose to **S**tandardize the data.\n",
    "    \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Smoot:</b> \n",
    "The smoot is a nonstandard, humorous unit of length created as part of an MIT fraternity prank. It is named after Oliver R. Smoot (...), who in October 1958 laid down repeatedly on the Harvard Bridge  (between Boston and Cambridge, Massachussetts) so that his fraternity brothers could use his height to measure the length of the bridge.<br><br>\n",
    "     <a href=\"https://en.wikipedia.org/wiki/Smoot\">Source</a> of this useless information :-)\n",
    "</div>\n",
    "\n",
    "<br><br>\n",
    "<font size=5 color=#009999> 3.1 - DATA IMPORT </font> <br>\n",
    "<font size=4 color=#009999>\n",
    "ONE NUMPY ARRAY PER UNITS\n",
    "</font> <br> <br>\n",
    "</font> \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 4 </b> <br>\n",
    "Create <b>numpy arrays</b> containing the features of interests. Here, we will consider that the <i>target variable</i>, $y$, is a binary variable that equals <b>zero</b> if the <code>OverallQual</code> is evaluated to five or less, and <b>one</b> otherwise. <br>\n",
    "You may want to have a look to pandas' <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html\">preprocessing</a> package for Stanley's data.\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "data = pd.read_csv(\"house_prices.csv\")\n",
    "data = data.head(50)\n",
    "\n",
    "# Creating features in Smoots and Bitcoins ($1 is 10000 XBT)\n",
    "data[\"LotAreaB\"]   = data[\"LotArea\"] * 0.0320782\n",
    "data[\"SalePriceB\"] = data[\"SalePrice\"] / 10000\n",
    "\n",
    "# Feature matrix (numpy.ndarray) of the three friends\n",
    "X_data_A = np.array(data[[\"LotArea\", \"SalePrice\"]])\n",
    "X_data_B = np.array(data[[\"LotAreaB\", \"SalePriceB\"]])\n",
    "\n",
    "\n",
    "# TODO: Create a numpy array with the features of interests standardized \n",
    "X_data_S = ... #To modify\n",
    "\n",
    "\n",
    "# TODO: Create a numpy array containing the targets\n",
    "y = np.round(np.random.rand(50)) #To modify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size=5 color=#009999> 3.2 - MODELS </font> <br>\n",
    "<font size=4 color=#009999>\n",
    "FIT A K-NN MODEL\n",
    "</font> <br> <br>\n",
    "</font> \n",
    "\n",
    "A possible approach comparing the quality of respectively  **A**, **B** and **S** choice, is the computation of the percentage of well-classified data. To evaluate a method, you could train a model and assess the training quality. But in this case, a \"method\" that simply memorized the data would have a perfect score but is unable to generalize. Yet, your goal is to classify data with unknown label. Therefore, you should test the model on _unseen_ data (i.e. that was not in the training set). It is therefore useful to split your dataset into two parts: one for training and one for testing.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 5 </b> <br>\n",
    "In order to get familiar with sklearn philosophy, let's start with a simple exercise: we implemented for you a (dummy) separation in train and test sets. \n",
    "<ol>\n",
    "   <li> Use the train set to <b>fit</b> a K-NN (with default parameter), \n",
    "   <li> <b>predict</b> the result of your fitted model on the test set and finally,\n",
    "   <li> compute the <b>score</b> of your model. \n",
    "</ol>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Dummy separation into train and test sets\n",
    "n_a, _  = X_data_A.shape\n",
    "\n",
    "n_train = n_a // 2\n",
    "n_test  = n_a - n_train\n",
    "\n",
    "X_train = X_data_A[0:n_train, :]\n",
    "y_train = y[0:n_train]\n",
    "X_test  = X_data_A[n_train:, :]\n",
    "y_test  = y[n_train:]\n",
    "\n",
    "\n",
    "# TODO: create your K-NN classifier\n",
    "clf = ...\n",
    "\n",
    "\n",
    "# TODO: fit your classifier on your train set\n",
    "\n",
    "\n",
    "# TODO: predict the output of the test set\n",
    "y_pred = np.ones(n_test) #To modify\n",
    "\n",
    "\n",
    "print('True test values are', np.array(y_test))\n",
    "print('Predicted values are', y_pred)\n",
    "\n",
    "# Here we simply compute the score :)\n",
    "score = np.sum(np.equal(y_pred, np.array(y_test)))/n_test\n",
    "\n",
    "print('Percentage of well-classified points ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 6 </b> <br>\n",
    "Now, create a function <code>fit_model(X, y, clf)</code> that takes as arguments: the data, the objective/target, and a classifier. This function trains the classifier on 70% of the data and tests it on the remaining 30% (You should use <a href=https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html>train_test_split</a> for train/test set separation). \n",
    "<code>fit_model</code> should return the classifier, the training data and objective, the testing data and objective as well as the score obtained by the classifier. \n",
    "</div> \n",
    "\n",
    "In the packages `helping_functions`, we implemented for you `vis_clf`, a function that allows to visualize the results of a classifier provided the data (see the signature of the function for more details).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\" -----------------------------------------------------------------------------------------\n",
    "Split the data into training (70%) and test (30%) set and fit the model using the training set\n",
    "INPUT: \n",
    "    - X: initial features\n",
    "    - y: initial objective\n",
    "    - clf: classifier\n",
    "OUTPUT:\n",
    "    - clf: fitted classifier\n",
    "    - X_train: training set\n",
    "    - y_train: training objective\n",
    "    - X_test: test set\n",
    "    - y_test: test objective\n",
    "    - score: mean accuracy on the test data.\n",
    "----------------------------------------------------------------------------------------- \"\"\"\n",
    "def fit_model(X, y, clf):   \n",
    " \n",
    "    # split between train and test sets\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n",
    "    score = 0\n",
    "    \n",
    "    # TODO: Build Model\n",
    "    \n",
    "    return clf, X_train, y_train, X_test, y_test, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 7 </b> <br>\n",
    "Create a model with 5 neighbors and distance-based weights and visualize its performances, you can use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">KNeighborsClassifier</a> from <code>sklearn</code>.\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helping_functions import vis_clf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Create labels\n",
    "names    = ['Alice\\'s imperial units', 'Bob\\'s smoots', 'Stanley\\'s scaled']\n",
    "x_labels = ['Lot area [sq. ft]', 'Lot area [smoot^2]', 'Scaled lot area']\n",
    "y_labels = ['Price [USD]', 'Price [XBT]', 'Scaled Price']\n",
    "\n",
    "# TODO: Build model\n",
    "n_neighbors = 5\n",
    "clf         = None \n",
    "\n",
    "# Create a dictionary X_dic with `names` as keys and your data as values.\n",
    "X_dic = {'Alice\\'s imperial units': X_data_A, 'Bob\\'s smoots': X_data_B, 'Stanley\\'s scaled': X_data_S}\n",
    "\n",
    "for (name, x_label, y_label) in zip(names, x_labels, y_labels):    \n",
    "    X = X_dic[name]\n",
    "    # TFit your model yo your data with a test set of 30% and a random_state of 42    \n",
    "    clf, X_train, y_train, X_test, y_test, score = fit_model(X, y, clf)\n",
    "    # Use vis_clf to vizualize the performances of your classifier\n",
    "    vis_clf(clf, X, X_train, y_train, X_test, y_test, score, n_neighbors, name,\n",
    "            \"Scaled lot area\", \"Scaled Price\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 8 </b> <br>\n",
    "Compare the three plots, and explain the differences. Which of the three versions would you choose?<br>\n",
    "Play with the parameter `n_neighbors`, how does the plots change?<br>\n",
    "Rerun the code on the whole data, and interpret the results for the scaled version.\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>FEEDBACK</b>  <br>\n",
    "\n",
    "As seen below, the general trends of the three plots are completely different. It reflects that the data scaling has a siginificative impact on the prediction. This property is unwanted... We would like our model to work independently of the data unit. <br>\n",
    "<ol>\n",
    "    <li> <b>Alice</b><br>\n",
    "        The separation between the two classes (blue and red) is mostly influenced by the price. A small change in the price affects a lot the predicted house quality. \n",
    "       <img src=\"FeedbackImgs/3-2_A.png\" width = \"500\">\n",
    "    <li> <b>Bob</b><br>\n",
    "        The separation between the two classes (blue and red) is mostly influenced by the lot area. A small change in the lot area affects a lot the predicted house quality. \n",
    "       <img src=\"FeedbackImgs/3-2_B.png\" width = \"500\">\n",
    "    <li> <b>Stanley</b><br>\n",
    "        Both features are scaled in the same way (the price varies approx. between -2 and 2 and the area varies approx. between -2 and 2). It seems that both features will impact the final prediction.\n",
    "       <img src=\"FeedbackImgs/3-2_C.png\" width = \"500\">\n",
    "</ol>\n",
    "<br>\n",
    "    \n",
    "    \n",
    "If we compare the 3 graphs, the decision boundary of Stanley is the smoothest. The two other decisions boudaries are sharp which can possibly indicates that the model is overfitting. Also, the score (predicted on the test set) on Stanley's data is the best. <br><br>\n",
    "    \n",
    "    \n",
    "The number of neighbors has also an impact on the classification. If we fix n_neighbors=1, we assign to each new data the label of the closest sample in the database. Meaning that if there is a misclassified sample, all new data that are close to this sample will be misclassified. The model is too sensitive to noise. <br>\n",
    "On the contrary, if we fix n_neighbors=n_samples (the number of training samples), the model will always predict the same label which is the most reprensented in the dataset. The model is not sensitive to the data.<br><br>\n",
    "    \n",
    "<b> Remark:</b> You may obtain slightly different results depending on the train/test set split. If you want to reproduce the same plots than ours, fix the parameter random_state=42 of the train_test_split function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size=5 color=#009999> 4 - DATA COMPLETION </font> <br>\n",
    "</font> \n",
    "<font size=4 color=#009999>\n",
    "HOW CAN WE HANDLE NANs?\n",
    "</font> <br> <br>\n",
    "\n",
    "Finally, Eckelman could invest in a third feature: the `LotFrontage`. \n",
    "- `LotFrontage`: the amount of streets connected to property in feet\n",
    "\n",
    "Unfortunately, this feature is incomplete and cannot be used directly. To improve your algorithm you have therefore two choices: either you neglect the incomplete data, or you try to fill them with clever inputs, in this case the mean or the median.\n",
    "\n",
    "Note that you should also use the two features from previous section: `SalePrice` and `LotArea`.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 9 </b> <br>\n",
    "Before implementing these two choices, let's analyze the feature <code>LotFrontage</code>: visualize the statistics of this variable using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html\">pandas.describe </a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html\">pandas.boxplot </a>. Do you think that there is a better approach <i>a priori</i>?\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "# For visualization purpose, we work with the 50 first observations\n",
    "data = pd.read_csv(\"house_prices.csv\")\n",
    "data = data.head(50)\n",
    "\n",
    "# Print the description of the new feature and (box) plot it\n",
    "dataLF = data[\"LotFrontage\"]\n",
    "print(dataLF.describe())\n",
    "data.boxplot(column=\"LotFrontage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>FEEDBACK</b>  <br>\n",
    "There are 8 missing values out of 50 samples, which is quite important. It is likely that discarding 16% of the data to train our model could have an impact on the performance. <br>\n",
    "Mean and Median are quite similar. We do not expect to observe big differences between replacing NANs values with the mean vs. with the median. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size=5 color=#009999> 4.1 - \"KEEP IT ALL OR DROP IT ALL\" APPROACH </font> <br>\n",
    "</font> \n",
    "<font size=4 color=#009999>\n",
    "THE GARBAGE\n",
    "</font> <br> <br>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 10 </b> <br>\n",
    "In this first approach, you will neglect data with missing values. To do so, remove all data with missing <code>LotFrontage</code>, and indicate how much data you are removing. \n",
    "\n",
    "You can use the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\">dropna</a> function from pandas. <br>\n",
    "    Use functions created in section 3.2 (<code>fit_model</code> and <code>vis_clf</code>) to create a function, `fit_vis`, that fits and visualizes the data. Observe your results. Do not forget to standardize your data! Is your classifier better than in previous section? Why is it so?\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def fit_vis(X_data, y, n_neighbors, str_method):\n",
    "    \"\"\" -----------------------------------------------------------------------------------------\n",
    "    Create a K-NN classifier, fit it and compute its score using fit_model, and visualize it using vis_clf\n",
    "    INPUT:\n",
    "        - X_data: features \n",
    "        - y: objective\n",
    "        - n_neihbors: number of neighbors used in the K-NN\n",
    "        - str_method: name of the method used to tackle NA values\n",
    "    OUTPUT:\n",
    "        - VOID: Visualization of the performance of the K-NN model fitted and tested using X_data.\n",
    "    ----------------------------------------------------------------------------------------- \"\"\"\n",
    "    \n",
    "    clf = ... # TODO: create your classifier\n",
    "    clf, X_train, y_train, X_test, y_test, score = fit_model(X_data_KD, y_KD, clf)\n",
    "\n",
    "    vis_clf(clf, X_data_KD, X_train, y_train, X_test, y_test, score, n_neighbors, str_method,\n",
    "            \"Scaled lot area\", \"Scaled Price\")\n",
    "    \n",
    "    print('Tackling NA values with:', str_method)\n",
    "    \n",
    "\n",
    "data2     = data[['LotArea', 'SalePrice', 'LotFrontage','OverallQual']]\n",
    "data_KD   = ... # TODO: use dropna to drop data with missing values (modify data2)\n",
    "X_data_KD = np.array(data_KD[[\"LotArea\", \"SalePrice\", \"LotFrontage\"]]) \n",
    "# Should you standardize your features?\n",
    "y_KD      = ... # TODO: obtain the binary target vector\n",
    "\n",
    "n_neighbors = 5\n",
    "fit_vis(X_data_KD, y_KD, n_neighbors, \"Keep it all or drop it all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> 4.2 - FILLING APPROACH </font> <br>\n",
    "<font size=4 color=#009999> MEAN </font> <br>\n",
    "</font> \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 11 </b> <br>\n",
    "Now, we will try to fill the missing values with the mean of <code>Lot Frontage</code>. For this purpose, you should use the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\">fillna</a>  and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html\">mean</a> functions of pandas. Do the completion before the standardization.  \n",
    "\n",
    "\n",
    "Again, using `fit_vis` observe and comment what you obtain in this case. \n",
    "</div> \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "data_Mean   = ... # TODO: use fillna to fill missing values (modify data2)\n",
    "X_data_Mean = np.array(data_Mean[[\"LotArea\", \"SalePrice\", \"LotFrontage\"]]) \n",
    "# Should you standardize your features?\n",
    "\n",
    "n_neighbors = 5\n",
    "fit_vis(X_data_Mean, y, n_neighbors, \"Filling with the mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> 4.3 - FILLING APPROACH </font> <br>\n",
    "<font size=4 color=#009999> MEDIAN </font> <br>\n",
    "</font> \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 12 </b> <br>\n",
    "Finally, we try to fill the missing values with the median of `Lot Frontage`. For this purpose, you should use the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\">fillna</a>  and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.median.html\">median</a> functions of pandas. \n",
    "\n",
    "Again, using `fit_vis` observe and comment what you obtain in this case. Was your hypothesis about the best approach for data completion correct? \n",
    "</div> \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> \n",
    "There exist other ways to fill the missing data, using for example information about the neighboring points, but this is out of the scope of this course. :) \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "data_Median   = ... # TODO: use fillna to fill missing values (modify data2)\n",
    "X_data_Median = np.array(data_Median[[\"LotArea\", \"SalePrice\", \"LotFrontage\"]]) \n",
    "# Should you standardize your features?\n",
    "\n",
    "n_neighbors = 5\n",
    "fit_vis(X_data_Median, y, n_neighbors, \"Filling with the median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>FEEDBACK</b>  <br>\n",
    "As expected, the performances obtained with the \"keep it all or drop it all\" approach is worse than replacing missing values with either the mean or the meadian (0.77 vs 0.93). It indicades that usefull information was contained in rows with NANs. <br>\n",
    "Here, 3D plots are not sufficient to conlude anything. The visualization in 3D is hard to interpret. \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
